{"cells":[{"cell_type":"markdown","metadata":{"id":"kiN_sPfz94Z2"},"source":["# Deep Learning for Media final Project\n","\n","### Weather Condition Classification Using Image Recognition\n","\n","---\n","Weather Condition Classification Using Image Recognition\n","\n","Part 1: Organizing our Data, Rita\n","\n","Part 2: Building, training, and evaluating baseline,Rita\n","\n","Part 3: Regularize the model, Data Augmentation\n","\n","Part 4: Building and using a pretrained model,\n","\n","Part 5: Data Analysis,\n","\n","Part 6: Confusion Matrix\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1745042472358,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"z2mNUmlaoUCa"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"tXbHhKmjOvkv"},"source":["### Part 1 & 2\n","In this project, we are tackling the classification of weather conditions using deep learning techniques on image data. Part 1: Organizing our Data involved loading and preparing the dataset, ensuring the images were properly structured for training and testing. This was followed by Part 2: Building, Training, and Evaluating the Baseline Model, where I developed a basic Convolutional Neural Network (CNN) for classifying weather conditions, trained it on the dataset, and evaluated its performance. The model's performance showed a significant improvement in test accuracy after five epochs. Moving forward, additional steps such as regularization, using pretrained models, performing data analysis, and evaluating with confusion matrices will further enhance the systemâ€™s performance and robustness."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1745042478715,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"B2nI6NYiMCnr","outputId":"6a3ab0d8-596a-4a35-a94d-7aa620c92643"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1745042479603,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"rUYpDPiMgtKu","outputId":"affaa228-44cd-4cd7-9d41-2b6033df5715"},"outputs":[],"source":["# Comment this out or don't run this block if you don't need to redirct, I got a module not found so I need this here -alison\n","%cd /content/drive/MyDrive/dl4m/dl4m-group7-main"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1745042480536,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"5dqf0BilMK3U"},"outputs":[],"source":["# Change the path to point to the folder in your Google Drive\n","# data_dir = '/content/drive/MyDrive/dl4m/dl4m-group7-main/data'\n","data_dir = './data' # use if running locally\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194977,"status":"ok","timestamp":1745042676508,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"rvLpkuvXJCSc","outputId":"2e291b73-4079-4873-d5da-6902f87b5a8d"},"outputs":[],"source":["# main.py or a Colab cell\n","\n","import torch\n","from project import load_data  # Import your data loading function\n","from utils import BaselineCNN, train_model, evaluate_model  # Import model and helper functions\n","\n","# Mount Google Drive to access data (only needed in Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load data (make sure to update the data path accordingly to your Google Drive structure)\n","data_dir = '/content/drive/MyDrive/dl4m/dl4m-group7-main/data'  # Update this with your folder path in Google Drive\n","train_loader, test_loader, classes = load_data(data_dir=data_dir)\n","num_classes = len(classes)\n","\n","# Initialize model, loss function, and optimizer\n","model = BaselineCNN(num_classes).to(device)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train and evaluate the model\n","num_epochs = 5  # You can adjust the number of epochs here\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","    # Train the model\n","    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n","\n","    # Evaluate the model\n","    test_loss, accuracy = evaluate_model(model, test_loader, criterion, device)\n","\n","    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296810,"status":"ok","timestamp":1745043743805,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"nZovRaGtnC1V","outputId":"54a85a8d-a22d-4b16-d70d-9cdc0655b97b"},"outputs":[],"source":["# Part 3 - Model Regularization\n","\n","# Import regularized model\n","import models as m\n","\n","# Initialize regularized model, define optimizer\n","weather_model = m.weather_model(num_classes).to(device)\n","#criterion = torch.nn.CrossEntropyLoss()   Use the same one from previous block\n","weather_optimizer = torch.optim.Adam(weather_model.parameters(), lr=1e-4, weight_decay=1e-4)  # Update optimizer with weight decay\n","\n","# Create empty lists to store results for plotting later\n","train_losses = []\n","test_losses = []\n","accuracies = []\n","\n","# Train and evaluate, store results\n","epochs = 8\n","for epoch in range(epochs):\n","  print(f\"Epoch {epoch+1}/{epochs}\")\n","  train_loss = train_model(weather_model, train_loader, criterion, weather_optimizer, device)\n","  train_losses.append(train_loss) # Store train loss\n","  test_loss, accuracy = evaluate_model(weather_model, test_loader, criterion, device)\n","  test_losses.append(test_loss)  # Store test loss\n","  accuracies.append(accuracy)  # Store accuracy\n","  print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1745043751993,"user":{"displayName":"Alison Yang","userId":"14527326377527012376"},"user_tz":240},"id":"726TEyPwkIEt","outputId":"954b7141-2416-4e42-b57a-d3a2f713d447"},"outputs":[],"source":["# Plot results\n","from utils import plot_loss\n","\n","plot_loss(train_losses, test_losses, accuracies, epochs)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Epoch 1/5\n"]},{"ename":"NameError","evalue":"name 'criterion' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(pretrained_model, train_loader, \u001b[43mcriterion\u001b[49m, optimizer, device)\n\u001b[1;32m     40\u001b[0m pretrained_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"]}],"source":["#  Part 4 - Using a pretrained model\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from utils import create_pretrained_model\n","from utils import train_model, evaluate_model\n","from project import load_data  # Import your data loading function\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# Set the data directory\n","data_dir = './data'  # Use local data path\n","\n","# Load the data first\n","train_loader, test_loader, classes = load_data(data_dir=data_dir)\n","num_classes = len(classes)\n","\n","# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Initialize the pre-trained model\n","pretrained_model = create_pretrained_model(num_classes).to(device)\n","\n","# Define loss function\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer - only train the classifier layer\n","optimizer = torch.optim.Adam(pretrained_model.fc.parameters(), lr=0.001)\n","\n","# Create empty lists to store results for plotting later\n","pretrained_train_losses = []\n","pretrained_test_losses = []\n","pretrained_accuracies = []\n","\n","# Train and evaluate the pre-trained model\n","pretrained_epochs = 5\n","for epoch in range(pretrained_epochs):\n","    print(f\"Epoch {epoch+1}/{pretrained_epochs}\")\n","    \n","    # Train the model\n","    train_loss = train_model(pretrained_model, train_loader, criterion, optimizer, device)\n","    pretrained_train_losses.append(train_loss)\n","    \n","    # Evaluate the model\n","    test_loss, accuracy = evaluate_model(pretrained_model, test_loader, criterion, device)\n","    pretrained_test_losses.append(test_loss)\n","    pretrained_accuracies.append(accuracy)\n","    \n","    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot results for the pre-trained model\n","from utils import plot_loss\n","\n","plot_loss(pretrained_train_losses, pretrained_test_losses, pretrained_accuracies, pretrained_epochs, \n","          title_prefix=\"Pre-trained Model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compare all models\n","import matplotlib.pyplot as plt\n","\n","models_accuracy = {\n","    'Baseline CNN': max(accuracies),  # From your Part 2\n","    'Regularized CNN': max(accuracies),  # From your Part 3\n","    'Pre-trained (Feature Extraction)': max(pretrained_accuracies),\n","    # 'Pre-trained (Fine-tuned)': max(fine_tuned_accuracies)  # If you implemented fine-tuning\n","}\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(models_accuracy.keys(), models_accuracy.values(), color=['blue', 'green', 'orange', 'red'])\n","plt.ylabel('Accuracy (%)')\n","plt.title('Model Comparison')\n","plt.ylim(0, 100)\n","for i, v in enumerate(models_accuracy.values()):\n","    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
